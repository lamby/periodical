#!/usr/bin/env python3

import os
import re
import sys
import json
import time
import click
import jinja2
import shutil
import pickle
import hashlib
import logging
import requests
import tempfile
import itertools
import subprocess

from xdg.BaseDirectory import save_cache_path

re_widont_html = re.compile(
    r'([^<>\s])\s+([^<>\s]+\s*)(</?(?:address|blockquote|br|dd|div|dt|fieldset|form|h[1-6]|li|noscript|p|td|th)[^>]*>|$)',
    re.IGNORECASE,
)

DEFAULT_EXCLUDE = {
    "Chess",
    "Chess puzzle",
    "Crossword",
    "Crossword solution",
    "The turf",
    "Drink",
    "Exhibitions",
    "Theatre",
    "Bridge",
    "Battle for Britain",
    "Music",
    "Arts feature",
    "Competition",
    "The Listener",
    "Wild life",
    "Pop",
    "Dance",
    "Opera",
    "Spectator Wine",
}


@click.command()
@click.argument('api_key', default='bltf04078f3cf7a9c30')
@click.argument('access_token', default='cs4b204a44bc4e88da701dbc9b')
@click.option(
    '--filename', default='', help="target filename (default: auto-generated)"
)
@click.option(
    '--keep-html',
    is_flag=True,
    default=False,
    help="Keep (and print) the HTML sources directory",
)
@click.option(
    '--include', multiple=True, help="include the specified sections"
)
@click.option(
    '--exclude', multiple=True, help="Exclude the specified sections"
)
@click.option(
    '--verbosity',
    type=int,
    default=1,
    help="Verbosity level; 0=minimal output, 1=normal output, "
    "2=verbose output, 3=very verbose output",
)
def main(*args, **kwargs):
    return SpectatorToMobi(*args, **kwargs).main()


class SpectatorToMobi:
    def __init__(
        self,
        api_key,
        access_token,
        filename,
        keep_html,
        include,
        exclude,
        verbosity,
    ):
        self.epoch = time.time() - 3600
        self.session = requests.Session()

        self.base_url = "https://spectator.co.uk"
        self.api_key = api_key
        self.access_token = access_token

        self.filename = filename
        self.keep_html = keep_html
        self.verbosity = verbosity

        self._parsed = None

        self.exclude = DEFAULT_EXCLUDE.copy()
        for x in exclude:
            self.exclude.update(set(x.split(',')))

        for x in include:
            for y in x.split(','):
                self.exclude.discard(y)

        # Normalise to lower-case
        self.exclude = {x.lower() for x in self.exclude}

    def main(self):
        self.context = {'articles': []}

        self.setup_logging()

        try:
            self.handle_base()
        except:
            if self._parsed:
                with tempfile.NamedTemporaryFile(prefix='spectator-to-mobi-', mode="w", suffix='.json', delete=False) as f:
                    json.dump(self._parsed, f, indent=2)
                    self.log.exception("Exception caught; writing last output to %s", f.name)
            raise

        t = tempfile.mkdtemp(prefix='spectator-to-mobi-')

        try:
            self.generate_mobi(t)
        finally:
            if self.keep_html:
                self.log.info("Keeping HTML in %s/index.html", t)
            else:
                shutil.rmtree(t, ignore_errors=True)

        return 0

    def handle_base(self):
        self.log.info("Starting conversion of %s", self.base_url)

        data = self.parse(
            "/content_types/magazine_issue/entries?environment=live&desc=issue_date&limit=1&only[BASE][]=url"
        )

        url = data['entries'][0]['url']
        date = self.context['date'] = url.split("/")[-1]

        self.log.debug("Latest magazine for %s is at %s", date, url)

        data = self.parse(
            "/content_types/article/entries?environment=live&include[]=topic&include[]=magazine_content_production_only.magazine_issue&include[]=magazine_content_production_only.magazine_subsection&include[]=author&query={%22magazine_content_production_only.magazine_issue%22:%20{%22$in_query%22:%20{%20%22url%22:%20%22"
            + url
            + "%22%20},%20%22_content_type_uid%22:%20%22magazine_issue%22},%20%22_content_type_uid%22:%20%22article%22}"
        )

        self.context['cover'] = data['entries'][0][
            "magazine_content_production_only"
        ]["magazine_issue"][0]["magazine_cover"]["url"]
        self.log.debug("Found cover at %s", self.context['cover'])

        for x in data['entries']:
            self.handle_article(x)

        # Pull to front
        for idx, x in enumerate(list(self.context['articles'])):
            if x['subsection'] == "Portrait of the week":
                self.context['articles'].pop(idx)
                self.context['articles'].insert(0, x)
        for idx, x in enumerate(list(self.context['articles'])):
            if x['subsection'] == "Leading article":
                self.context['articles'].pop(idx)
                self.context['articles'].insert(1, x)

        # Push to end
        for idx, x in enumerate(list(self.context['articles'])):
            if x['subsection'] == "Food":
                self.context['articles'].pop(idx)
                self.context['articles'].append(x)
                break
        for idx, x in enumerate(list(self.context['articles'])):
            if x['subsection'] == "Dear Mary":
                self.context['articles'].pop(idx)
                self.context['articles'].append(x)
                break

    def handle_article(self, data):
        author = ', '.join(x['title'] for x in data['author'])

        # Title
        title = data['title']

        # If the title starts with the author's name, strip it out.
        title = title.replace("{}: ".format(author), "")

        byline = data['byline']
        subsection = data['magazine_content_production_only'][
            'magazine_subsection'
        ][0]['title']

        try:
            image = data['hero_image'][0]["url"]
        except IndexError:
            image = None  # Hero images are optional

        # Subsection

        # Rewrite first
        if subsection.startswith("More from "):
            subsection = subsection[10:]
        if subsection == "Lead book review":
            subsection = "Books"
        if subsection == "Notes on...":
            subsection = "Notes on"

        # Exclude
        if subsection.lower() in self.exclude:
            return

        if subsection in ("Letters", "Dear Mary", "Portrait of the week"):
            title = subsection
            byline = ""

        # Author

        if author == "The Spectator":
            author = ""

        # Byline

        # If byline is missing a full-stop, append one. We match for a
        # lower-case letter, to avoid whack-a-mole with various other
        # punctuation.
        if re.search(r'[a-z]$', byline or ''):
            byline = "{}.".format(byline)

        # Body
        body = data['text_body']

        # Add book metadata
        for x in reversed(data['books_arts']):
            body = '<p>{}: <em>{}</em></p>{}'.format(
                x['books_title'], x['books_author'], body
            )

        def cb_widont(m):
            return '{}&nbsp;{}{}'.format(*m.groups())

        body = re_widont_html.sub(cb_widont, body)

        for pattern, repl in ((r'\s*CSH</p>', ''),):
            body = re.sub(pattern, repl, body)


        data = {
            'title': title,
            'byline': byline,
            'author': author,
            'url': 'https://www.spectator.co.uk{}'.format(data['url']),
            'idx': len(self.context['articles']),
            'body': body,
            'image': image,
            'subsection': subsection,
        }

        self.context['articles'].append(data)

    def generate_mobi(self, tempdir):
        self.log.info("Generating .mobi in %s", tempdir)

        with open(os.path.join(tempdir, 'context.json'), 'w') as f:
            json.dump(self.context, f, indent=2)

        assert self.context[
            'articles'
        ], "No articles downloaded; please check {}".format(self.base_url)

        template_dir = os.path.join(os.path.dirname(__file__), 'templates')
        env = jinja2.Environment(loader=jinja2.FileSystemLoader(template_dir))

        self.context['tempdir'] = tempdir
        self.context['grouped'] = [
            (x, list(y))
            for x, y in itertools.groupby(
                self.context['articles'], lambda x: x['subsection']
            )
        ]

        self.save_url_to(
            self.context['cover'], os.path.join(tempdir, 'cover.jpg')
        )

        for x in ('index.html', 'toc.html', 'style.css'):
            val = env.get_template(x).render(**self.context)
            with open(os.path.join(tempdir, x), 'w') as f:
                f.write(val)

        # Download article images
        for x in self.context['articles']:
            if x['image'] is None:
                continue

            target = os.path.join(tempdir, '{}.jpg'.format(x['idx']))
            self.save_url_to(x['image'], target)

        # Hide kindlegen output by default
        stdout, stderr = subprocess.PIPE, subprocess.PIPE
        if self.verbosity >= 2:
            stdout, stderr = None, None

        subprocess.call(
            (
                'kindlegen/kindlegen',
                '-verbose',
                os.path.join(tempdir, 'index.html'),
            ),
            stdout=stdout,
            stderr=stderr,
        )

        if not self.filename:
            self.filename = 'The_Spectator_{}.mobi'.format(
                self.context['date'].replace(' ', '_')
            )

        self.log.info("Saving output to %s", self.filename)

        shutil.move(os.path.join(tempdir, 'index.mobi'), self.filename)

    def parse(self, url, **kwargs):
        url = 'https://cdn.contentstack.io/v3{}'.format(url)

        self._parsed = self.get(url, **kwargs).json()

        return self._parsed

    def get(self, url, **kwargs):
        kwargs.update(
            {'api_key': self.api_key, 'access_token': self.access_token}
        )

        h = hashlib.sha1()
        h.update(url.encode('utf-8'))
        for k, v in kwargs.items():
            h.update(k.encode('utf-8'))
            h.update(v.encode('utf-8'))

        filename = os.path.join(
            save_cache_path('spectator-to-mobi'), h.hexdigest()
        )

        if (
            os.path.exists(filename)
            and os.path.getmtime(filename) > self.epoch
        ):
            with open(filename, 'rb') as f:
                return pickle.load(f)

        response = self.session.get(
            url, headers={'User-agent': 'Mozilla/5.0'}, params=kwargs
        )
        response.raise_for_status()

        with open(filename, 'wb') as f:
            pickle.dump(response, f)

        return response

    def save_url_to(self, url, target):
        with open(target, 'wb') as f:
            for x in self.get(url).iter_content(chunk_size=128):
                f.write(x)

    def setup_logging(self):
        self.log = logging.getLogger()
        self.log.setLevel(
            {0: logging.WARNING, 1: logging.INFO, 2: logging.DEBUG}[
                self.verbosity
            ]
        )

        handler = logging.StreamHandler(sys.stderr)
        handler.setFormatter(
            logging.Formatter('%(asctime).19s %(levelname).1s %(message)s')
        )
        self.log.addHandler(handler)

    def json_find(self, needle, haystack):
        if not isinstance(haystack, dict):
            return

        for k, v in haystack.items():
            if k == needle:
                yield v

            elif isinstance(v, dict):
                for x in self.json_find(needle, v):
                    yield x

    def json_find_first(self, needle, haystack):
        return list(self.json_find(needle, haystack))[0]


if __name__ == '__main__':
    sys.exit(main())
