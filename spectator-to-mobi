#!/usr/bin/env python3

import os
import re
import sys
import json
import time
import click
import jinja2
import shutil
import pickle
import logging
import requests
import tempfile
import itertools
import subprocess

from lxml import etree
from lxml.cssselect import CSSSelector
from xdg.BaseDirectory import save_cache_path

re_filename_safe = re.compile(r'\w')


@click.command()
@click.argument('url', default='https://www.spectator.co.uk/magazine/')
@click.argument('filename', default='')
@click.option('--keep-html', is_flag=True, default=False,
              help="Keep (and print) the HTML sources directory")
@click.option('--verbosity',
              type=int,
              default=1,
              help="Verbosity level; 0=minimal output, 1=normal output, "
                   "2=verbose output, 3=very verbose output")
def main(*args, **kwargs):
    SpectatorToMobi(*args, **kwargs).main()


class SpectatorToMobi(object):
    def __init__(self, url, filename, keep_html, verbosity):
        self.epoch = time.time() - 3600
        self.session = requests.Session()

        self.base_url = url
        self.filename = filename
        self.keep_html = keep_html
        self.verbosity = verbosity

    def main(self):
        self.context = {
            'articles': [],
        }

        self.setup_logging()
        self.handle_base()

        t = tempfile.mkdtemp()

        try:
            self.generate_mobi(t)
        finally:
            if self.keep_html:
                self.log.info("Keeping HTML in %s", t)
            else:
                shutil.rmtree(t, ignore_errors=True)

        return 0

    def handle_base(self):
        self.log.info("Starting conversion of %s", self.base_url)

        html = self.parse(self.base_url)

        self.context['date'] = \
            CSSSelector('.issue-details__date')(html)[0].text.strip()

        self.context['cover'] = \
            CSSSelector('a.issue-details__cover-link')(html)[0].get('href')

        for x in CSSSelector('.term-item a.term-item__title-link')(html):
            self.handle_article(x.get('href'))

        # Pull to front
        for idx, x in enumerate(list(self.context['articles'])):
            if x['category'] == "Portrait of the week":
                self.context['articles'].pop(idx)
                self.context['articles'].insert(0, x)
                break

    def handle_article(self, url):
        html = self.parse(url)

        def text(x):
            try:
                elem = CSSSelector(x)(html)[0]
            except IndexError:
                return None

            return ' '.join(elem.itertext()).strip()

        author = text('.article-header .article-header__author a')
        title = text('.article-header h1')
        subtitle = text('.article-header h2')
        category = text('.article-header .article-header__category a')

        if category in (
            "Crossword",
            "Crossword solution",
            "Competition",
            "Chess puzzle",
            "Spectator Wine",
        ):
            return

        # If the title starts with the author's name, strip it out.
        title = title.replace("{}: ".format(author), "")

        # Remove some subtitles by pushing them into the title
        for x in (
            "Portrait of the Week",
            "Barometer",
        ):
            if subtitle is not None and "{}: ".format(x) in subtitle:
                title = x
                subtitle = ""
                category = x

        if author == "The Spectator":
            author = ""

        # Fixup the leading article author
        if category == "Leading article":
            author = "The Spectator"

        # Recategorise some things
        for x, y in (
            ("Lead book review", "Books"),
            ("Leading book review", "Books"),
            ("From The Archives", "From the Archives"),
            ("Battle for Britain", "The Battle for Britain"),
            ("Notes on...", "Notes on"),
        ):
            if category == x:
                category = y

        # If subtitle is missing a full-stop, append one. We match for a
        # lower-case letter, to avoid whack-a-mole with various other
        # punctuation.
        if re.search(r'[a-z]$', subtitle or ''):
            subtitle = "{}.".format(subtitle)

        if category in (
            "Letters",
            "Dear Mary",
            "Portrait of the week",
        ):
            title = category
            subtitle = ""

        try:
            image = CSSSelector('.featured-image img')(html)[0].get('src')
        except IndexError:
            image = None

        # Parse/tidy body
        elem = CSSSelector('.article-body .ev-meter-content-class')(html)[0]
        etree.strip_tags(elem, 'div', 'iframe', 'script', 'v-email-newsletter')

        body = etree.tostring(elem, encoding='unicode')

        for pat, repl in (
            (r'\u00a0', '&nbsp;'),
            (r'\ngoogletag.*', ''),
            (r'<p/>', ''),
            (r'<h2>', '<h5>'),
            (r'</h2>', '</h5>'),
            (r'<h3>', '<h6>'),
            (r'</h3>', '</h6>'),
            (r'<p><strong>.*Spectator Podcast.</strong></p>', ''),
        ):
            body = re.sub(pat, repl, body)

        if category == "Portrait of the week":
            body = re.sub(r'\.\s*[A-Z]{3}', '.', body)

        data = {
            'title': title,
            'subtitle': subtitle,
            'author': author,

            'url': url,
            'idx': len(self.context['articles']),

            'body': body,
            'image': image,
            'category': category,
        }

        self.context['articles'].append(data)

    def generate_mobi(self, tempdir):
        self.log.info("Generating .mobi in %s", tempdir)

        with open(os.path.join(tempdir, 'context.json'), 'w') as f:
            json.dump(self.context, f, indent=2)

        assert self.context['articles'], \
            "No articles downloaded; please check {}".format(self.base_url)

        template_dir = os.path.join(os.path.dirname(__file__), 'templates')
        env = jinja2.Environment(loader=jinja2.FileSystemLoader(template_dir))

        self.context['tempdir'] = tempdir
        self.context['grouped'] = [(x, list(y)) for x, y in itertools.groupby(
            self.context['articles'],
            lambda x: x['category'],
        )]

        self.save_url_to(
            self.context['cover'],
            os.path.join(tempdir, 'cover.jpg'),
        )

        for x in ('index.html', 'toc.html', 'style.css'):
            val = env.get_template(x).render(**self.context)
            with open(os.path.join(tempdir, x), 'w') as f:
                f.write(val)

        # Download article images
        for x in self.context['articles']:
            if x['image'] is None:
                continue

            target = os.path.join(tempdir, '{}.jpg'.format(x['idx']))
            self.save_url_to(x['image'], target)

        # Hide kindlegen output by default
        stdout, stderr = subprocess.PIPE, subprocess.PIPE
        if self.verbosity >= 2:
            stdout, stderr = None, None

        subprocess.call((
            'kindlegen/kindlegen',
            '-verbose',
            os.path.join(tempdir, 'index.html'),
        ), stdout=stdout, stderr=stderr)

        target = self.filename
        if not self.filename:
            target = 'The_Spectator_{}.mobi'.format(
                self.context['date'].replace(' ', '_'),
            )

        self.log.info("Saving output to %s", target)

        shutil.move(os.path.join(tempdir, 'index.mobi'), target)

    def parse(self, *args, **kwargs):
        return etree.HTML(self.get(*args, **kwargs).text)

    def get(self, url):
        filename = os.path.join(
            save_cache_path('spectator-to-mobi'),
            ''.join(x for x in url if re_filename_safe.match(x)),
        )

        if os.path.exists(filename) and os.path.getmtime(filename) > self.epoch:
            with open(filename, 'rb') as f:
                return pickle.load(f)

        response = self.session.get(url, headers={'User-agent': 'Mozilla/5.0'})
        response.raise_for_status()

        with open(filename, 'wb') as f:
            pickle.dump(response, f)

        return response

    def save_url_to(self, url, target):
        with open(target, 'wb') as f:
            for x in self.get(url).iter_content(chunk_size=128):
                f.write(x)

    def setup_logging(self):
        self.log = logging.getLogger()
        self.log.setLevel({
            0: logging.WARNING,
            1: logging.INFO,
            2: logging.DEBUG,
        }[self.verbosity])

        handler = logging.StreamHandler(sys.stderr)
        handler.setFormatter(
            logging.Formatter('%(asctime).19s %(levelname).1s %(message)s')
        )
        self.log.addHandler(handler)


if __name__ == '__main__':
    main()
